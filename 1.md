1. **分词：对句子中的单词近行编码（传统编码是对字符编码，现在变成对整个单词进行编码）,可以用API Tokenizer快速实现**


   ```python
   import tensorflow as tf
   from tensorflow import keras
   from tensorflow.keras.preprocessing.text import Tokenizer

   sentence=['I love my dog', 'I love my cat']  #词库
   tokenizer=Tokenizer(num_words=100)   #构造分词器，并设定保留频率前100的词进行分析
   tokenizer.fit_on_texts(sentence)  #分词器和词库的绑定
   w=tokenizer.word_index   #分词结果，是作为一个属性值存在
   print(w)
   ```
   **结果：**
    `{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}`
    **到这一步，实现了编码，这个编码的规则本质可以不了解，反正就是一个单词有一个编码即可。==并且这个编码时考虑到词频等因素的，不是随机生成的！==**
    注意：例如`I love my dog！` 此工具会自动识别并排除标点符号，也就是会给dog编号，而不是dog！（**注意输入法，如果是中文输入法下，其实它不能识别，所以并不会自动排除**）

2. **序列化：在分词编码基础上，将文本转化为数字序列**
    ```py
      import tensorflow as tf
      from tensorflow import keras
      from tensorflow.keras.preprocessing.text import Tokenizer

      sentence=['I love my dog!', 'I love my cat','You love my dog','Do you think my dog is amazing']  #词库
      tokenizer=Tokenizer(num_words=100)   #构造分词器，并设定保留频率前100的词进行分析，注意python通过特殊方法，使得这种形式，为指定的属性赋值。
      tokenizer.fit_on_texts(sentence)  #分词器和句子库库的绑定
      w=tokenizer.word_index   #分词结果，是作为一个属性值存在
      print(w)

      #序列化：
      sequence=tokenizer.texts_to_sequences(sentence)  #将句子库传入此方法，生成一个序列，是句子库每个句子对应的编码序列
      print(sequence)

    ```
    结果：
     `{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}`
`[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]
     `
     **注意：每个分词器（实际也是编码器）实际和最初的训练集是绑定的，当分词器通过fit_on_texts绑定训练集后，在进行序列化时，其中的句子有最初训练集中不包含的单词，默认情况下序列化结果就是当这个词不存在，这种处理过于粗糙。**
     **改进：构造分词器时，为其设定成员oov_token的值，表示此分词器会给该值进行编码（常为1），当使用分词器遇到训练集不存在的词时，编码时视为oov字符进行编码。**
     例如：
      ```java
       import tensorflow as tf
       from tensorflow import keras
       from tensorflow.keras.preprocessing.text import Tokenizer

       sentence=['I love my dog!', 'I love my cat','You love my dog','Do you think my dog is amazing']  #词库
       tokenizer=Tokenizer(num_words=100 ,oov_token="<OOV>")   #构造分词器，并设定保留频率前100的词进行分析,为oov_token赋值，赋什么其实不重要，主要是表示不再对未出现的词直接粗暴丢弃。
       tokenizer.fit_on_texts(sentence)  #分词器和句子库库的绑定
       w=tokenizer.word_index   #确定的编码结果，是作为一个属性值存在
       print(w)

       #序列化：
       sequence=tokenizer.texts_to_sequences(['You is a pig'])  #将句子库传入此方法，生成一个序列，是句子库每个句子对应的编码序列
       print(sequence)

      ```
      结果：
      `{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}`
      `[[6, 10, 1, 1]]`
3. **填充：序列化后得到的向量显然由于句子长短不同而不同，为了便于处理我们要用0进行填充使得所有句子的长度一样。**
   首先导入：
  `from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences`
然后把序列化的结果sequence传给方法pad_sequences，即可得到填充后的序列，例如：
   ```py
    sequence=tokenizer.texts_to_sequences(['You is  pig','I love your dog','I love i you dog aha aa'])  #将句子库传入此方法，生成一个序列，是句子库每个句子对应的编码序列
    padded=pad_sequences(sequence)
    print(padded)
   ```
   结果：
   ```
   [[ 0  0  0  0  6 10  1]
    [ 0  0  0  5  3  1  4]
    [ 5  3  5  6  4  1  1]]
   ```
   **默认根据最长句子填充，默认填充在前面，可以通过参数传入进行改变：**
   `padded=pad_sequences(sequence,padding='post',maxlen=8)` 例如这里通过改成post变成在后面填充，maxlen表示需要填充的长度，注意此时可能句子长度超过maxlen，因此会出现截断丢弃，此时可以再补充参数truncating='pre'或者'post'，'pre'表示丢弃前面，'post'表示丢弃后面。

4. 案例（情感分析）
   一个json形式的数据文件，每个元素是一个新闻，其中包含新闻标题，新闻url，以及标题讽刺与否（1是0否），python可以将json文件转换为一个列表，每个列表元素为一个新闻（**嵌套容器**）
    1. 导入json文件：
       先补充文件打开知识：python的文件读取的基本形式是：
        ```py
         try:
         f = open('/path/to/file', 'r')
         print(f.read())
         finally:
         if f:
         f.close()
        ```
        python还提供更简化的写法：
        `with open('/path/to/file', 'r') as f:`
         &emsp; `print(f.read())`
         **回到这里，加载json文件：**
          ```py
          import json
          with open ('sarcasm.json.','r') as f:    #json放到和py文件一个目录下
          datastore=json.load(f)               #把json文件的内容加载到变量datastore中

          #datastore是嵌套容器，可以对其迭代，将其信息分离
          sentence=[]
          labels=[]
          urls=[]

          for item in datastore:
          sentence.append(item['headline'])
          labels.append(item['is_sarcastic'])
          urls.append(item['article_link'])
          ```
          但是运行了一下跑不通，实际需要修改：因为load文件不止一行，所以不能直接用load加载读取文件
          修改：**先将json文件一行一行读入（readlines实现一行一行全部读入，放到容器中），将读入后的json内容用loads（load用于json文件，loads用于json格式信息，这里已经读入了，所以是信息而不是整个文件）转换为python格式，再分离放入新容器。**
         ```py
            import json
            file = open('sarcasm.json', 'r' )            #把json文件的内容加载到变量datastore中
            #datastore是嵌套容器，可以对其迭代，将其信息分离
            sentence=[]
            labels=[]
            urls=[]
            for line in file.readlines():
              item=json.loads(line)
              sentence.append(item['headline'])
              labels.append(item['is_sarcastic'])
              urls.append(item['article_link'])
          ```

      2. **对得到的标题容器（句子库）进行之前学习的分词编码处理**
    ```py
        import json
        from tensorflow.keras.preprocessing.text import Tokenizer
        from tensorflow.keras.preprocessing.sequence import  pad_sequences
        file = open('sarcasm.json', 'r' )            #把json文件的内容加载到变量datastore中

        #datastore是嵌套容器，可以对其迭代，将其信息分离
        sentence=[] #放句子，也就是要分析的标题
        labels=[]  #方1/0，也就是是否是讽刺的
        urls=[]

        for line in file.readlines():
        item=json.loads(line)
        sentence.append(item['headline'])
        labels.append(item['is_sarcastic'])
        urls.append(item['article_link'])
 
        tokenizer =Tokenizer(oov_token="<OOV>")
        tokenizer.fit_on_texts(sentence)
        word_index=tokenizer.word_index

        sequences=tokenizer.texts_to_sequences(sentence)
        padded=pad_sequences(sequences,padding='post')
        print(padded[0])       #显然第一个句子的编码转换结果
        print(padded.shape)    #会显示出句子的总数，以及每个句子的长度
     ```
4. **分片：将得到的sequence分成两份，一份用于训练集，一份作为用于检验的测试集，但是这里有个问题，如果先分词编码，然后再分成训练集和测试集，那么分词器分配编码时，见过了全部数据，也就是见过了测试集，这是不合理的，所以我们希望分词器只见过训练集。（==测试集只能在最终全部模型成型后，作为输入和模型交互==），因此必须先对句子库进行分片分成两个集，然后对测试集进行编码。**
例如：（**看注释，搞清楚！**）
```py
training_sentences=sentence[0:20000]  #前两万用作测试
testing_sentences=sentence[20000:]

#构造分词器，并确认训练集，定下此分词器的编码规则（同一个分词器确认编码一次，但可以多次序列化句子，这是一个工具而已！）
tokenizer =Tokenizer( oov_token="<OOV>")
tokenizer.fit_on_texts(training_sentences )
word_index=tokenizer.word_index   #哪个词对应哪个数

#获得训练集的句子转换为的数字序列（并填充）：
training_sequences=tokenizer.texts_to_sequences(training_sentences)
training_padded=pad_sequences(training_sequences ,maxlen=40,padding='post')
 
#获得测试集的句子转换为的数字序列（并填充）：
testing_sequences=tokenizer.texts_to_sequences(testing_sentences)
testing_padded=pad_sequences(testing_sequences ,maxlen=40,padding='post')
 
```

6. **情感判断（嵌入 ）**
   以最简单的二维判断好坏来入门，如果（1，0）代表好，（-1，0）代表坏，那么任意一个词，我们认为他在好坏判断中，有一个坐标（极坐标），我们把一句话中全部的单词的向量进行横相加、纵相加后，得到这个句子的总向量，如果其角度偏向（1，0），则认为这句话偏向好，如果偏向（-1，0），则认为这句话偏向坏。
   **而实际考虑到更多因素，因此用多维向量，==神经网络可以帮助我们得到每个词在讨论某个问题时（比如讽刺与否）的多维向量==，最终我们把一句话中的向量全部求和，看总向量的偏向，进而得到我们对这句话的判断，**


7.   本质来讲，自动写作就是对写一个单词的预测，一般来讲，越近的单词对推测下一个单词的作用越大（短期记忆），但这不必然，有的时候决定下一个单词的，是某个在很早提到的单词决定，所以短期记忆不完备，需要长短期记忆结合（**LSTM**）